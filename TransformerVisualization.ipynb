{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Your Own Visualizations!\n",
    "Instructions:\n",
    "1. Install tensor2tensor and train up a Transformer model following the instruction in the repository https://github.com/tensorflow/tensor2tensor.\n",
    "2. Update cell 3 to point to your checkpoint, it is currently set up to read from the default checkpoint location that would be created from following the instructions above.\n",
    "3. If you used custom hyper parameters then update cell 4.\n",
    "4. Run the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NeuralSum as ns\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.bin import t2t_decoder  # To register the hparams set\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.visualization import attention\n",
    "from tensor2tensor.visualization import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min'\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT THE MODEL YOU WANT TO LOAD HERE!\n",
    "CHECKPOINT = os.path.expanduser('./data/tensor2tensor/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HParams\n",
    "problem_name = 'summary_problem'\n",
    "data_dir = os.path.expanduser('./data/tensor2tensor/data')\n",
    "model_name = \"my_custom_transformer\"\n",
    "hparams_set = \"exp_6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizer = visualization.AttentionVisualizer(hparams_set, model_name, data_dir, problem_name, beam_size=1)\n",
    "\n",
    "EOS_ID = 1\n",
    "class AttentionVisualizer(object):\n",
    "    \"\"\"Helper object for creating Attention visualizations.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "          self, hparams_set, model_name, data_dir, problem_name, beam_size=1):\n",
    "        inputs, targets, samples, att_mats = build_model(\n",
    "            hparams_set, model_name, data_dir, problem_name, beam_size=beam_size)\n",
    "\n",
    "        # Fetch the problem\n",
    "        ende_problem = problems.problem(problem_name)\n",
    "        encoders = ende_problem.feature_encoders(data_dir)\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.att_mats = att_mats\n",
    "        self.samples = samples\n",
    "        self.encoders = encoders\n",
    "\n",
    "    def encode(self, input_str):\n",
    "        \"\"\"Input str to features dict, ready for inference.\"\"\"\n",
    "        inputs = self.encoders['inputs'].encode(input_str) + [EOS_ID]\n",
    "        batch_inputs = np.reshape(inputs, [1, -1, 1, 1])  # Make it 3D.\n",
    "        return batch_inputs\n",
    "\n",
    "    def decode(self, integers):\n",
    "        \"\"\"List of ints to str.\"\"\"\n",
    "        integers = list(np.squeeze(integers))\n",
    "        return self.encoders['inputs'].decode(integers)\n",
    "\n",
    "    def decode_list(self, integers):\n",
    "        \"\"\"List of ints to list of str.\"\"\"\n",
    "        integers = list(np.squeeze(integers))\n",
    "        return self.encoders['inputs'].decode_list(integers)\n",
    "\n",
    "    def get_vis_data_from_string(self, sess, input_string):\n",
    "        encoded_inputs = self.encode(input_string)\n",
    "\n",
    "        # Run inference graph to get the translation.\n",
    "        out = sess.run(self.samples, {\n",
    "            self.inputs: encoded_inputs,\n",
    "        })\n",
    "\n",
    "        # Run the decoded translation through the training graph to get the\n",
    "        # attention tensors.\n",
    "        att_mats = sess.run(self.att_mats, {\n",
    "            self.inputs: encoded_inputs,\n",
    "            self.targets: np.reshape(out, [1, -1, 1, 1]),\n",
    "        })\n",
    "\n",
    "        output_string = self.decode(out)\n",
    "        input_list = self.decode_list(encoded_inputs)\n",
    "        output_list = self.decode_list(out)\n",
    "\n",
    "        return output_string, input_list, output_list, att_mats\n",
    "\n",
    "def build_model(hparams_set, model_name, data_dir, problem_name, beam_size=1):\n",
    "    hparams = trainer_lib.create_hparams(\n",
    "      hparams_set, data_dir=data_dir, problem_name=problem_name)\n",
    "    translate_model = registry.model(model_name)(\n",
    "      hparams, tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "    inputs = tf.placeholder(tf.int32, shape=(1, None, 1, 1), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(1, None, 1, 1), name='targets')\n",
    "    translate_model({\n",
    "      'inputs': inputs,\n",
    "      'targets': targets,\n",
    "    })\n",
    "    att_mats = get_att_mats(translate_model)\n",
    "\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "        samples = translate_model.infer({\n",
    "            'inputs': inputs,\n",
    "        }, beam_size=beam_size)['outputs']\n",
    "\n",
    "    return inputs, targets, samples, att_mats\n",
    "\n",
    "def get_att_mats(translate_model):\n",
    "    enc_atts = []\n",
    "    dec_atts = []\n",
    "    encdec_atts = []\n",
    "\n",
    "    prefix = 'my_custom_transformer/body/' # had to change this from the original\n",
    "    postfix = '/multihead_attention/dot_product_attention'\n",
    "\n",
    "    for i in range(translate_model.hparams.num_hidden_layers):\n",
    "        enc_att = translate_model.attention_weights[\n",
    "            '%sencoder/layer_%i/self_attention%s' % (prefix, i, postfix)]\n",
    "        dec_att = translate_model.attention_weights[\n",
    "            '%sdecoder/layer_%i/self_attention%s' % (prefix, i, postfix)]\n",
    "        encdec_att = translate_model.attention_weights[\n",
    "            '%sdecoder/layer_%i/encdec_attention%s' % (prefix, i, postfix)]\n",
    "        enc_atts.append(enc_att)\n",
    "        dec_atts.append(dec_att)\n",
    "        encdec_atts.append(encdec_att)\n",
    "\n",
    "    return enc_atts, dec_atts, encdec_atts\n",
    "\n",
    "visualizer = AttentionVisualizer(hparams_set, model_name, data_dir, problem_name, beam_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Variable(0, dtype=tf.int64, trainable=False, name='global_step')\n",
    "\n",
    "sess = tf.train.MonitoredTrainingSession(\n",
    "    checkpoint_dir=CHECKPOINT,\n",
    "    save_summaries_secs=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"what started as a local controversy in salt lake city has evolved into a full-blown international scandal .\"\n",
    "output_string, inp_text, out_text, att_mats = visualizer.get_vis_data_from_string(sess, input_sentence)\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Visualizations\n",
    "- The layers drop down allow you to view the different Transformer layers, 0-indexed of course.\n",
    "  - Tip: The first layer, last layer and 2nd to last layer are usually the most interpretable.\n",
    "- The attention dropdown allows you to select different pairs of encoder-decoder attentions:\n",
    "  - All: Shows all types of attentions together. NOTE: There is no relation between heads of the same color - between the decoder self attention and decoder-encoder attention since they do not share parameters.\n",
    "  - Input - Input: Shows only the encoder self-attention.\n",
    "  - Input - Output: Shows the decoder’s attention on the encoder. NOTE: Every decoder layer attends to the final layer of encoder so the visualization will show the attention on the final encoder layer regardless of what layer is selected in the drop down.\n",
    "  - Output - Output: Shows only the decoder self-attention. NOTE: The visualization might be slightly misleading in the first layer since the text shown is the target of the decoder, the input to the decoder at layer 0 is this text with a GO symbol prepreded.\n",
    "- The colored squares represent the different attention heads.\n",
    "  - You can hide or show a given head by clicking on it’s color.\n",
    "  - Double clicking a color will hide all other colors, double clicking on a color when it’s the only head showing will show all the heads again.\n",
    "- You can hover over a word to see the individual attention weights for just that position.\n",
    "  - Hovering over the words on the left will show what that position attended to.\n",
    "  - Hovering over the words on the right will show what positions attended to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention.show(inp_text, out_text, *att_mats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
